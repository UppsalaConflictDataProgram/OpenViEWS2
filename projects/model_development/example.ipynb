{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New model interface example\n",
    "\n",
    "This notebook aims to show how to use the new ViEWS2 modelling interface.\n",
    "\n",
    "Todo:\n",
    "* Map plot at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging imports\n",
    "import json\n",
    "import logging\n",
    "import views\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    #level=logging.INFO, # uncomment this and comment debug above for less yelling in red\n",
    "    format=views.config.LOGFMT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASETS is a dictionary of Dataset objects.\n",
    "from views import DATASETS\n",
    "# These are the building blocks of the modelling interface\n",
    "from views import Ensemble, Model, Downsampling, Period\n",
    "# These are model specifications from the specfiles\n",
    "from views.specs.models import cm as model_specs_cm, pgm as model_specs_pgm\n",
    "from views.specs.periods import get_periods, get_periods_by_name\n",
    "# Utils\n",
    "from views.utils import db, io, data as datautils\n",
    "from views.utils.data import assign_into_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the core models defined in the ViEWS pipeline\n",
    "# These are defined in \n",
    "from views.apps.pipeline.models_cm import all_cm_models_by_name\n",
    "from views.apps.pipeline.models_pgm import all_pgm_models_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the available datasets we have specified\n",
    "for name, dataset in DATASETS.items():\n",
    "    print(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you wish to fetch the latest public data? If so, change False to True and run this cell\n",
    "# Cells below will fail if this is not run if you haven't imported data yourself yet.\n",
    "if False:\n",
    "    path_zip = views.apps.data.public.fetch_latest_zip_from_website(path_dir_destination=views.DIR_SCRATCH)\n",
    "    views.apps.data.public.import_tables_and_geoms(tables=views.TABLES, geometries=views.GEOMETRIES, path_zip=path_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe for a particular dataset. \n",
    "# If it doesn't exist cached on your machine it will be fetched from db and transforms computed for you\n",
    "# Datasets are defined in views/specs/data/\n",
    "dataset = views.DATASETS[\"cm_africa_imp_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change False to True to rebuild this datasest if you have updated tables\n",
    "if False:\n",
    "    dataset.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual dataframe\n",
    "df = dataset.df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for looking up periods\n",
    "# d is for development\n",
    "run_id = \"d_2020_04_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Periods are defined as simple Period objects with 4 attributes, train/predict_start/end\n",
    "# Models expect a list of Periods so they know which training times to use\n",
    "# and which times to make predictions for\n",
    "periods = get_periods(run_id) # as a list\n",
    "periods_by_name = get_periods_by_name(run_id) # as a dict\n",
    "period_a = periods_by_name[\"A\"]\n",
    "period_b = periods_by_name[\"B\"]\n",
    "period_c = periods_by_name[\"C\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also define periods yourself\n",
    "period_custom = Period(name=\"A\", train_start=201, train_end=396, predict_start=397, predict_end=432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [1, 12, 24, 36]\n",
    "downsampling_10pct = Downsampling(share_positive=1.0, share_negative=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_a = [\"time_since_ged_dummy_sb\", \"time_since_ged_dummy_ns\", \"time_since_ged_dummy_os\"]\n",
    "my_model = Model(\n",
    "    name = \"cm_sb_mymodel\", \n",
    "    col_outcome = \"greq_25_ged_best_sb\", \n",
    "    cols_features = features_a,\n",
    "    steps = steps,\n",
    "    periods = periods,\n",
    "    outcome_type = \"prob\",\n",
    "    estimator = RandomForestClassifier(n_jobs=-1, n_estimators=100),\n",
    "    tags=[\"sb\"]\n",
    ")\n",
    "\n",
    "my_downsampled_model = Model(\n",
    "    name = \"cm_sb_mymodel_downsampled\", \n",
    "    col_outcome = \"greq_25_ged_best_sb\", \n",
    "    cols_features = features_a,\n",
    "    steps = steps,\n",
    "    periods = periods,\n",
    "    outcome_type = \"prob\",\n",
    "    downsampling=downsampling_10pct,\n",
    "    estimator = RandomForestClassifier(n_jobs=-1, n_estimators=100),\n",
    "    tags=[\"sb\", \"downsampled\"]\n",
    ")\n",
    "\n",
    "# Notice how the col_outcome is the same\n",
    "# The model itself does the onset transformation\n",
    "# and subsets the training data itself\n",
    "# Also transforms outcome col for calibration\n",
    "# Evaluation and predicting not affected by onset yet\n",
    "my_onsetmodel = Model(\n",
    "    name = \"cm_sb_onset_mymodel\",\n",
    "    col_outcome = \"greq_25_ged_best_sb\",\n",
    "    cols_features = features_a,\n",
    "    steps=steps,\n",
    "    periods=periods,\n",
    "    outcome_type = \"prob\",\n",
    "    estimator = RandomForestClassifier(n_jobs=-1, n_estimators=100),\n",
    "    onset_outcome=True, # <-- Onset switch\n",
    "    onset_window=24, # <-- Must be accompanied by an onset time window\n",
    "    tags=[\"sb\", \"onset\"]\n",
    ")\n",
    "\n",
    "my_delta_model = Model(\n",
    "    name=\"delta_force\",\n",
    "    col_outcome=\"ged_best_sb\",\n",
    "    cols_features=features_a,\n",
    "    steps=steps,\n",
    "    periods=periods,\n",
    "    outcome_type=\"real\",\n",
    "    delta_outcome=True,\n",
    "    tags=[\"delta\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models can also be loaded from the definition in the pipeline\n",
    "print(all_cm_models_by_name.keys())\n",
    "model_from_pipeline_spec = all_cm_models_by_name[\"cm_sb_icgcw\"]\n",
    "# Just printing the object should show everything we care about\n",
    "model_from_pipeline_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of models are convenient\n",
    "models = [my_model, my_downsampled_model, my_onsetmodel, model_from_pipeline_spec]\n",
    "models = [my_model, my_downsampled_model, my_onsetmodel]\n",
    "\n",
    "\n",
    "my_avg_ensemble = Ensemble(\n",
    "    name=\"my_avg_ensemble\", \n",
    "    models=models, \n",
    "    outcome_type=\"prob\", \n",
    "    col_outcome=\"greq_25_ged_best_sb\", \n",
    "    method=\"average\", \n",
    "    periods=periods\n",
    ")\n",
    "my_ebma_ensemble = Ensemble(\n",
    "    name=\"my_ebma_ensemble\", \n",
    "    models=models, \n",
    "    outcome_type=\"prob\", \n",
    "    col_outcome=\"greq_25_ged_best_sb\", \n",
    "    method=\"ebma\", \n",
    "    periods=periods\n",
    ")\n",
    "\n",
    "ensembles = [my_avg_ensemble]\n",
    "# When R installation and EBMA setup are part of the standard installer:\n",
    "# include the EBMA ensemble here\n",
    "#ensembles = [my_avg_ensemble, my_ebma_ensemble]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit estimator for their specified steps and periods \n",
    "# Estimators are stored on disk with only a reference in the model object\n",
    "# This could be omitted after the first run of the notebook\n",
    "for model in models:\n",
    "    model.fit_estimators(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and store predictions for their specified steps and periods in df\n",
    "for model in models:\n",
    "    \n",
    "    # Uncalibrated predictions\n",
    "    df_pred = model.predict(df)\n",
    "    # assign_into_df takes care to only overwrite rows with actual values\n",
    "    # This way we can keep all periods in the same df\n",
    "    # It's also idempotent, no joining, so run as many times as you like. \n",
    "    df = assign_into_df(df_to=df, df_from=df_pred)\n",
    "    \n",
    "    # Calibrated predictions\n",
    "    df_pred = model.predict_calibrated(\n",
    "        df=df, \n",
    "        period_calib=period_a,\n",
    "        period_test=period_b,\n",
    "    )\n",
    "    df = assign_into_df(df_to=df, df_from=df_pred)\n",
    "    df_pred = model.predict_calibrated(\n",
    "        df=df, \n",
    "        period_calib=period_b,\n",
    "        period_test=period_c,\n",
    "    )\n",
    "    df = assign_into_df(df_to=df, df_from=df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ensemble in ensembles:\n",
    "    df_pred = ensemble.predict(\n",
    "        df=df,\n",
    "        period_calib=period_a,\n",
    "        period_test=period_b,\n",
    "    )\n",
    "    df = assign_into_df(df_to=df, df_from=df_pred)\n",
    "    df_pred = ensemble.predict(\n",
    "        df=df,\n",
    "        period_calib=period_b,\n",
    "        period_test=period_c,\n",
    "    )\n",
    "    df = assign_into_df(df_to=df, df_from=df_pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "for model in models:\n",
    "    model.evaluate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all ensembles, limit to B and C. \n",
    "# In future evaluate will figure out itself where it has predictions to evaluate and this will be just one call.\n",
    "for ensemble in ensembles:\n",
    "    ensemble.evaluate(df, period=periods_by_name[\"B\"])\n",
    "    ensemble.evaluate(df, period=periods_by_name[\"C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(model.name)\n",
    "    #print(model.scores)\n",
    "    print(\"EVAL SCORES:\")\n",
    "    print(json.dumps(model.scores, indent=2))\n",
    "    print(\"FEATURE_IMPORTANCES\")\n",
    "    print(json.dumps(model.extras.feature_importances, indent=2))\n",
    "    print(\"#\"*80)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the uncalibrated scores, they are identical to calibrated.\n",
    "# Evaluation needs a bit of a refactor\n",
    "for ensemble in ensembles:\n",
    "    print(ensemble.name)\n",
    "    print(\"Weights:\")\n",
    "    print(json.dumps(ensemble.weights, indent=2))\n",
    "    print(\"Eval scores:\")\n",
    "    print(json.dumps(ensemble.evaluation.scores, indent=2))\n",
    "    print(\"#\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access individual eval scores like a dict\n",
    "print(models[0].name)\n",
    "# Period B step 1\n",
    "models[0].scores[\"B\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice all features and predictions in the same dataframe, no more a/b/c \n",
    "# Instead we subset by the periods when needed\n",
    "\n",
    "cols_predict = [model.col_sc_calibrated for model in models] + [ensemble.col_sc for ensemble in ensembles]\n",
    "\n",
    "# All calibrated predictions for period C\n",
    "df.loc[period_c.times_predict, cols_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_a_1 = my_downsampled_model.estimators.get(period_name=\"A\", step=1)\n",
    "estimator_a_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_downsampled_model.extras.feature_importances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
